de0[,"meanelev"]
#
parm <- c(beta0 =-3.655905, beta1=0)
Data <- list(N= length(pi0),mon.names=c("LogPosterior"), parm.names=names(parm),
r0=pi0, n0=pi1, meanelev=de0[,"meanelev"])
beta0 <- parm[1]
beta1 <- parm[2]
r <- Data$r0
n <- Data$n0
x1 <- Data$meanelev- mean(Data$meanelev)
x1
Model <- function(parm, Data){
# parameters
beta0 <- parm[1]
beta1 <- parm[2]
r <- Data$r0
n <- Data$n0
x1 <- Data$meanelev- mean(Data$meanelev)
# log-likelihood
logit.pi <- beta0 + beta1*x1
pi <- invlogit(logit.pi)
LL <- sum(dbinom(x=r, size=n, prob=pi, log=T))
# priors
beta0.priors <- dnormv(beta0, 0.0, 1.0E3, log=T)
beta1.priors <- dnormv(beta1, 0.0, 1.0E3, log=T)
#
LP <- LL + beta0.priors + beta1.priors
#
Modelout <- list(LP=LP, Dev=-2*LL,  Monitor=c(LP),
yhat= rbinom(n=length(pi),size=n, prob=pi), parm=parm)
return(Modelout)
}
pi0 <- LGtab$X00枯立木
pi1 <- rowSums(LGtab[-c(1:3)])
de0 <-topo.10[, c("meanelev","meanelev2", "convex", "convex2",
"slope","slope2", "asp.cos", "asp.sin")]
de0 <- apply(de0, 2, as.numeric)
#
parm <- c(beta0 =-3.655905, beta1=0)
Data <- list(N= length(pi0),mon.names=c("LogPosterior"), parm.names=names(parm),
r0=pi0, n0=pi1, meanelev=de0[,"meanelev"])
Model(parm, Data)
#
out <- LaplacesDemon(Model, Data=Data, Initial.Values=parm,
Iterations=10000, Status=2000, Thinning=50)
#
out
plot(out, BurnIn=50, Data, PDF=F)
Data <- list(N= length(pi0),mon.names=c("LogPosterior"), parm.names=names(parm),
r0=pi0, n0=pi1, meanelev=de0[,"meanelev"])
#   Model(parm, Data)
#
#
out <- LaplacesDemon(Model, Data=Data, Initial.Values=parm,
Iterations=10000, Status=2000, Thinning=50)
#
out
#
plot(out, BurnIn=50, Data, PDF=F)
Iterations=100000, Status=2000, Thinning=50)
#
out
#
plot(out, BurnIn=50, Data, PDF=F)
out <- LaplacesDemon(Model, Data=Data, Initial.Values=parm,
Iterations=100000, Status=2000, Thinning=50)
#
out
#
plot(out, BurnIn=50, Data, PDF=F)
#
plot(out, BurnIn=50, Data, PDF=F)
Data <- list(N= length(pi0),mon.names=c("LogPosterior"), parm.names=names(parm),
r0=pi0, n0=pi1, meanelev=de0[,"meanelev"])
#   Model(parm, Data)
#
#
out <- LaplacesDemon(Model, Data=Data, Initial.Values=parm,
Iterations=100000, Status=2000, Thinning=50)
#
out
#
ls()
rm(ls())
rm(list=ls())
#
setwd("F:\\DataW\\DeadStandingTrees")
##
dea.10 <- read.csv("DeadData-10m-abun.csv")
topo.10 <- read.csv("LG.topographic 10m poly4 2013-9-4.csv")
LGtab <- read.csv('LGdata_table.csv')
pi0 <- LGtab$X00枯立木
pi1 <- rowSums(LGtab[-c(1:3)])
#
require(LaplacesDemon)
#
Model <- function(parm, Data){
# parameters
beta0 <- parm[1]
beta1 <- parm[2]
r <- Data$r0
n <- Data$n0
x1 <- Data$meanelev- mean(Data$meanelev)
# log-likelihood
logit.pi <- beta0 + beta1*x1
pi <- invlogit(logit.pi)
LL <- sum(dbinom(x=r, size=n, prob=pi, log=T))
# priors
beta0.priors <- dnormv(beta0, 0.0, 1.0E3, log=T)
beta1.priors <- dnormv(beta1, 0.0, 1.0E3, log=T)
#
LP <- LL + beta0.priors + beta1.priors
#
Modelout <- list(LP=LP, Dev=-2*LL,  Monitor=c(LP),
yhat= rbinom(n=length(pi),size=n, prob=pi), parm=parm)
return(Modelout)
}
#
pi0 <- LGtab$X00枯立木
pi1 <- rowSums(LGtab[-c(1:3)])
de0 <-topo.10[, c("meanelev","meanelev2", "convex", "convex2",
"slope","slope2", "asp.cos", "asp.sin")]
de0 <- apply(de0, 2, as.numeric)
dea.10 <- read.csv("DeadData-10m-abun.csv")
topo.10 <- read.csv("LG.topographic 10m poly4 2013-9-4.csv")
LGtab <- read.csv('LGdata_table.csv')
pi0 <- LGtab$X00枯立木
pi1 <- rowSums(LGtab[-c(1:3)])
de0 <-topo.10[, c("meanelev","meanelev2", "convex", "convex2",
"slope","slope2", "asp.cos", "asp.sin")]
de0 <- apply(de0, 2, as.numeric)
#
parm <- c(beta0 =-3.655905, beta1=10)
Data <- list(N= length(pi0),mon.names=c("LogPosterior"), parm.names=names(parm),
r0=pi0, n0=pi1, meanelev=de0[,"meanelev"])
beta0 <- parm[1]
beta1 <- parm[2]
r <- Data$r0
n <- Data$n0
r
Model <- function(parm, Data){
# parameters
beta0 <- parm[1]
beta1 <- parm[2]
r <- Data$pi0
n <- Data$pi1
x1 <- Data$meanelev- mean(Data$meanelev)
# log-likelihood
logit.pi <- beta0 + beta1*x1
pi <- invlogit(logit.pi)
LL <- sum(dbinom(x=r, size=n, prob=pi, log=T))
# priors
beta0.priors <- dnormv(beta0, 0.0, 1.0E3, log=T)
beta1.priors <- dnormv(beta1, 0.0, 1.0E3, log=T)
#
LP <- LL + beta0.priors + beta1.priors
#
Modelout <- list(LP=LP, Dev=-2*LL,  Monitor=c(LP),
yhat= rbinom(n=length(pi),size=n, prob=pi), parm=parm)
return(Modelout)
}
#
#
#
parm <- c(beta0 =-3.655905, beta1=10)
Data <- list(N= length(pi0),mon.names=c("LogPosterior"), parm.names=names(parm),
pi0=pi0, pi1=pi1, meanelev=de0[,"meanelev"])
# parameters
beta0 <- parm[1]
beta1 <- parm[2]
r <- Data$pi0
n <- Data$pi1
x1 <- Data$meanelev- mean(Data$meanelev)
# log-likelihood
logit.pi <- beta0 + beta1*x1
pi <- invlogit(logit.pi)
LL <- sum(dbinom(x=r, size=n, prob=pi, log=T))
# priors
beta0.priors <- dnormv(beta0, 0.0, 1.0E3, log=T)
beta1.priors <- dnormv(beta1, 0.0, 1.0E3, log=T)
#
LP <- LL + beta0.priors + beta1.priors
#
Modelout <- list(LP=LP, Dev=-2*LL,  Monitor=c(LP),
yhat= rbinom(n=length(pi),size=n, prob=pi), parm=parm)
Modelout
Model(parm, Data)
#
parm <- c(beta0 =-3.655905, beta1=10)
Data <- list(N= length(pi0),mon.names=c("LogPosterior"), parm.names=names(parm),
pi0=pi0, pi1=pi1, meanelev=de0[,"meanelev"])
#   Model(parm, Data)
#
#
out <- LaplacesDemon(Model, Data=Data, Initial.Values=parm,
Iterations=100000, Status=2000, Thinning=50)
#
out
#
as.data.frame(out$Summary2)$Mean[1:5]
as.data.frame(out$Summary2)$Mean
mean00 <- as.data.frame(out$Summary2)$Mean
beta0.mean <- mean00[1]
beta1.mean <- mean00[2]
mean00 <- as.data.frame(out$Summary2)$Mean
beta0.mean <- mean00[1]
beta1.mean <- mean00[2]
d <- length(mean00)-2
N <- length(pi0)
exp0 = exp(beta0.mean)
pi.mean = exp0/(1+exp0)
BIC = -2*log(prod(dbinom(x=pi0, size=pi1, prob=pi.mean)))+log(N)*d
print(c("BIC ", BIC))
#######################################################
rm(list=ls())
#
setwd("F:\\DataW\\DeadStandingTrees")
##
dea.10 <- read.csv("DeadData-10m-abun.csv")
topo.10 <- read.csv("LG.topographic 10m poly4 2013-9-4.csv")
LGtab <- read.csv('LGdata_table.csv')
pi0 <- LGtab$X00枯立木
pi1 <- rowSums(LGtab[-c(1:3)])
de0 <-topo.10[, c("meanelev","meanelev2", "convex", "convex2",
"slope","slope2", "asp.cos", "asp.sin")]
de0 <- apply(de0, 2, as.numeric)
#
require(LaplacesDemon)
#
Model <- function(parm, Data){
# parameters
beta0 <- parm[1]
beta1 <- parm[2]
r <- Data$pi0
n <- Data$pi1
X1 <- Data$x1- mean(Data$x1)
# log-likelihood
logit.pi <- beta0 + beta1*X1
pi <- invlogit(logit.pi)
LL <- sum(dbinom(x=r, size=n, prob=pi, log=T))
# priors
beta0.priors <- dnormv(beta0, 0.0, 1.0E3, log=T)
beta1.priors <- dnormv(beta1, 0.0, 1.0E3, log=T)
#
LP <- LL + beta0.priors + beta1.priors
#
Modelout <- list(LP=LP, Dev=-2*LL,  Monitor=c(LP),
yhat= rbinom(n=length(pi),size=n, prob=pi), parm=parm)
return(Modelout)
}
#
#
# ########################################
#
parm <- c(beta0 =-3.655905, beta1=10)
Data <- list(N= length(pi0),mon.names=c("LogPosterior"), parm.names=names(parm),
pi0=pi0, pi1=pi1, x1=de0[,"meanelev2"])
#   Model(parm, Data)
#
#
out <- LaplacesDemon(Model, Data=Data, Initial.Values=parm,
Iterations=50000, Status=2000, Thinning=50)
#
out
#
plot(out, BurnIn=50, Data, PDF=F)
Model(parm, Data)
parm <- c(beta0 =-3.655905, beta1=0)
Data <- list(N= length(pi0),mon.names=c("LogPosterior"), parm.names=names(parm),
pi0=pi0, pi1=pi1, x1=de0[,"meanelev2"])
#
Model(parm, Data)
#
#
out <- LaplacesDemon(Model, Data=Data, Initial.Values=parm,
Iterations=50000, Status=2000, Thinning=50)
#
out
#######################################################
rm(list=ls())
#
setwd("F:\\DataW\\DeadStandingTrees")
##
dea.10 <- read.csv("DeadData-10m-abun.csv")
topo.10 <- read.csv("LG.topographic 10m poly4 2013-9-4.csv")
LGtab <- read.csv('LGdata_table.csv')
pi0 <- LGtab$X00枯立木
pi1 <- rowSums(LGtab[-c(1:3)])
de0 <-topo.10[, c("meanelev","meanelev2", "convex", "convex2",
"slope","slope2", "asp.cos", "asp.sin")]
de0 <- apply(de0, 2, as.numeric)
#
require(LaplacesDemon)
#
Model <- function(parm, Data){
# parameters
beta0 <- parm[1]
beta1 <- parm[2]
r <- Data$pi0
n <- Data$pi1
X1 <- Data$x1- mean(Data$x1)
# log-likelihood
logit.pi <- beta0 + beta1*X1
pi <- invlogit(logit.pi)
LL <- sum(dbinom(x=r, size=n, prob=pi, log=T))
# priors
beta0.priors <- dnormv(beta0, 0.0, 1.0E3, log=T)
beta1.priors <- dnormv(beta1, 0.0, 1.0E3, log=T)
#
LP <- LL + beta0.priors + beta1.priors
#
Modelout <- list(LP=LP, Dev=-2*LL,  Monitor=c(LP),
yhat= rbinom(n=length(pi),size=n, prob=pi), parm=parm)
return(Modelout)
}
#
#
# ########################################
#
parm <- c(beta0 =-3.655905, beta1=0)
Data <- list(N= length(pi0),mon.names=c("LogPosterior"), parm.names=names(parm),
pi0=pi0, pi1=pi1, x1=de0[,"meanelev2"])
#
Model(parm, Data)
#
out <- LaplacesDemon(Model, Data=Data, Initial.Values=parm,
Iterations=100000, Status=2000, Thinning=50)
#
out
#
rm(list=ls())
require(LaplacesDemon)
#
Model <- function(parm, Data){
# parameters
alpha <- parm[1]
beta <- parm[2]
sigma <- exp(parm[3])
# priors distribution
alpha.priors <- dnormv(alpha, 0, 100, log=T)
beta.priors <- dnormv(beta, 0, 100, log=T)
sigma.priors <- dgamma(sigma, 25, log=T)
# log-likelihood
mu <- alpha + beta*Data$x + sigma
LL <- sum(dnorm(Data$y, mu, sigma, log=T))
# log-Posterior
LP <- LL + alpha.priors + beta.priors + sigma.priors
#
Modelout <- list(LP=LP, Dev=-2*LL,  Monitor=c(LP),
yhat= rnorm(length(mu), mu, sigma), parm=parm)
return(Modelout)
}
#
# ########################################
#
###  test data ~~~~~~~~~~~~~~
x.test = runif(100, -100, 100)
ee = rnorm(length(x.test),0,0.1)
y.test = 180 - 320*x.test +ee
parm0 <- c(alpha=0, beta=0, sigma=log(1))
PGF <- function(Data){c(rnormv(2,0,1000), log(rhalfcauchy(1,25)))}
Data <- list(N=length(x.test), PGF=PGF,
mon.names=c("LogPosterior"),
parm.names=names(parm0), x=x.test, y=y.test)
#
Initial.Values <- GIV(Model, MyData, PGF=TRUE)
###  test data ~~~~~~~~~~~~~~
x.test = runif(100, -100, 100)
ee = rnorm(length(x.test),0,0.1)
y.test = 180 - 320*x.test +ee
#parm0 <- c(alpha=0, beta=0, sigma=log(1))
PGF <- function(Data){c(rnormv(2,0,1000), log(rhalfcauchy(1,25)))}
MyData <- list(N=length(x.test), PGF=PGF,
mon.names=c("LogPosterior"),
parm.names=names(parm0), x=x.test, y=y.test)
#
Initial.Values <- GIV(Model, MyData, PGF=TRUE)
#
###  test data ~~~~~~~~~~~~~~
x.test = runif(100, -100, 100)
ee = rnorm(length(x.test),0,0.1)
y.test = 180 - 320*x.test +ee
#parm0 <- c(alpha=0, beta=0, sigma=log(1))
PGF <- function(Data){c(rnormv(2,0,1000), log(rhalfcauchy(1,25)))}
MyData <- list(N=length(x.test), PGF=PGF,
mon.names=c("LogPosterior"),
parm.names=names(parm0), x=x.test, y=y.test)
#
Initial.Values <- GIV(Model, MyData, PGF=TRUE)
#
###  test data ~~~~~~~~~~~~~~
x.test = runif(100, -100, 100)
ee = rnorm(length(x.test),0,0.1)
y.test = 180 - 320*x.test +ee
#parm0 <- c(alpha=0, beta=0, sigma=log(1))
PGF <- function(Data){c(rnormv(2,0,1000), log(rhalfcauchy(1,25)))}
MyData <- list(N=length(x.test), PGF=PGF,
mon.names=c("LogPosterior"),
parm.names=names(parm0), x=x.test, y=y.test)
#
Initial.Values <- GIV(Model, MyData, PGF=TRUE)
#
###  test data ~~~~~~~~~~~~~~
x.test = runif(100, -100, 100)
ee = rnorm(length(x.test),0,0.1)
y.test = 180 - 320*x.test +ee
#parm0 <- c(alpha=0, beta=0, sigma=log(1))
PGF <- function(Data){c(rnormv(2,0,1000), log(rhalfcauchy(1,25)))}
MyData <- list(N=length(x.test), PGF=PGF,
mon.names=c("LogPosterior"),
parm.names=names(parm0), x=x.test, y=y.test)
#
Initial.Values <- GIV(Model, MyData, PGF=TRUE)
#
#
##################  Differential Evolution Markov Chain  ##################
Fit1 <- LaplacesDemon(Model, Data=MyData, Initial.Values,
Covar=NULL, Iterations=1000, Status=100, Thinning=1,
Algorithm="DEMC", Specs=list(Nc=3, Z=NULL, gamma=NULL, w=0.1))
#######################  Hamiltonian Monte Carlo  #########################
Fit2 <- LaplacesDemon(Model, Data=MyData, Initial.Values,
Covar=NULL, Iterations=1000, Status=100, Thinning=1,
Algorithm="HMC", Specs=list(epsilon=rep(0.02, length(Initial.Values)),
L=2))
########################  Independence Metropolis  ########################
### Note: the mu and Covar arguments are populated from a previous Laplace
### Approximation.
Fit3 <- LaplacesDemon(Model, Data=MyData, Initial.Values,
Covar=Fit$Covar, Iterations=1000, Status=100, Thinning=1,
Algorithm="IM",
Specs=list(mu=Fit$Summary1[1:length(Initial.Values),1]))
######################  Robust Adaptive Metropolis  #######################
Fit4 <- LaplacesDemon(Model, Data=MyData, Initial.Values,
Covar=NULL, Iterations=1000, Status=100, Thinning=1,
Algorithm="RAM", Specs=list(alpha.star=0.234, Dist="N", gamma=0.66,
Periodicity=10))
Fit3<- Fit <- LaplacesDemon(Model, Data=MyData, Initial.Values,
Covar=Fit$Covar, Iterations=1000, Status=100, Thinning=1,
Algorithm="IM",
Specs=list(mu=Fit$Summary1[1:length(Initial.Values),1]))
Fit3<- Fit <- LaplacesDemon(Model, Data=MyData, Initial.Values,
Covar=Fit$Covar, Iterations=1000, Status=100, Thinning=1,
Algorithm="IM",
Specs=list(mu=Fit$Summary1[1:length(Initial.Values),1]))
######################  Robust Adaptive Metropolis  #######################
Fit4 <- LaplacesDemon(Model, Data=MyData, Initial.Values,
Covar=NULL, Iterations=1000, Status=100, Thinning=1,
Algorithm="RAM", Specs=list(alpha.star=0.234, Dist="N", gamma=0.66,
Periodicity=10))
###################  Tempered Hamiltonian Monte Carlo  ####################
Fit5 <- LaplacesDemon(Model, Data=MyData, Initial.Values,
Covar=NULL, Iterations=1000, Status=100, Thinning=1,
Algorithm="THMC", Specs=list(epsilon=rep(0.05,length(Initial.Values)),
L=2, Temperature=2))
#
Fit1
Fit1
Fit2
Fit3
Fit4
Fit5
###  test data ~~~~~~~~~~~~~~
x.test = runif(100, -100, 100)
ee = rnorm(length(x.test),0,0.1)
y.test = 180 - 320*x.test +ee
#parm0 <- c(alpha=0, beta=0, sigma=log(1))
PGF <- function(Data){c(rnormv(2,0,1000), log(rhalfcauchy(1,25)))}
MyData <- list(N=length(x.test), PGF=PGF,
mon.names=c("LogPosterior"),
parm.names=names(parm0), x=x.test, y=y.test)
#
Initial.Values <- GIV(Model, MyData, PGF=TRUE)
#
##################  Differential Evolution Markov Chain  ##################
Fit1 <- LaplacesDemon(Model, Data=MyData, Initial.Values,
Covar=NULL, Iterations=1000, Status=100, Thinning=1,
Algorithm="DEMC", Specs=list(Nc=3, Z=NULL, gamma=NULL, w=0.1))
#######################  Hamiltonian Monte Carlo  #########################
Fit2 <- LaplacesDemon(Model, Data=MyData, Initial.Values,
Covar=NULL, Iterations=1000, Status=100, Thinning=1,
Algorithm="HMC", Specs=list(epsilon=rep(0.02, length(Initial.Values)),
L=2))
########################  Independence Metropolis  ########################
### Note: the mu and Covar arguments are populated from a previous Laplace
### Approximation.
Fit3<- Fit <- LaplacesDemon(Model, Data=MyData, Initial.Values,
Covar=Fit$Covar, Iterations=1000, Status=100, Thinning=1,
Algorithm="IM",
Specs=list(mu=Fit$Summary1[1:length(Initial.Values),1]))
######################  Robust Adaptive Metropolis  #######################
Fit4 <- LaplacesDemon(Model, Data=MyData, Initial.Values,
Covar=NULL, Iterations=1000, Status=100, Thinning=1,
Algorithm="RAM", Specs=list(alpha.star=0.234, Dist="N", gamma=0.66,
Periodicity=10))
###################  Tempered Hamiltonian Monte Carlo  ####################
Fit5 <- LaplacesDemon(Model, Data=MyData, Initial.Values,
Covar=NULL, Iterations=1000, Status=100, Thinning=1,
Algorithm="THMC", Specs=list(epsilon=rep(0.05,length(Initial.Values)),
L=2, Temperature=2))
#
#
Fit1
Fit2
Fit3
Fit4
Fit5
Fit1
Fit2
Fit3
Fit4
Fit5
out <- Fit
